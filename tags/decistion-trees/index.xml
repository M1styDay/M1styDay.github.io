<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Decistion Trees on M1sty</title>
    <link>https://www.m1sty.com/tags/decistion-trees/</link>
    <description>Recent content in Decistion Trees on M1sty</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.m1sty.com/tags/decistion-trees/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Decision Trees：Content</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_1_content/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_1_content/</guid>
      <description>Decisision Trees Classification Definition  Given a collection of records (training set)  Each record contains a set of attributes, one of the attributes is the Class.   Find a model for Class attribute as a function of the values of other attributes Goal: previously unseen records should be assigned a Class as accurately as possible  A test set is used to determine the accuracy of the model. Usually, the given data set is divided into training and test sets, with training set used to build the model and test set used to validate it.</description>
    </item>
    
    <item>
      <title>Decision Trees：Exercise</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_2_exercise/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_2_exercise/</guid>
      <description>Decision Trees：Exercise Question Construct a decision tree with the following properties:
 Choose the best split based on GINI index; Stopping rule: when gini value=0 or no further split is possible; Class of a leaf is determined by majority rule. If ties use the default class (NM).  Answer Building Decision Tree Step 1 Step 2 Step 3 Predicting Post Pruning </description>
    </item>
    
    <item>
      <title>Decision Trees：More</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_4_more/</link>
      <pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_4_more/</guid>
      <description>Chapter 8  分类是一种数据分析形式,它提取描述数据类的模型。分类器或分类模型预测类别标号(类)。数值预测建立连续值函数模型。分类和数值预测是两类主要的预测问题。 决策树归纳是一种自顶向下递归树归纳算法,它使用一种属性选择度量为树的每个非树叶结点选择 属性测试。ID3、C4.5和CART都是这种算法的例子,它们使用不同的属性选择度量。树剪枝算法试图通过剪去反映数据中噪声的分枝,提高准确率。早期的决策树算法通常假定数据是驻留内存的。已经为可伸缩的树归纳提出了一些可伸缩的算法,如Rainforest。 朴素贝叶斯分类基于后验概率的贝叶斯定理。它假定类条件独立—一个属性值对给定类的影响独立于其他属性的值。 基于规则的分类器使用IF-THEN规则进行分类。规则可以从决策树提取,或者使用顺序覆盖算法直接由训练数据产生。 混淆矩阵可以用来评估分类器的质量。对于两类问题,它显示真正例、真负例、假正例、假负例。评估分类器预测能力的度量包括准确率、灵敏度(又称为召回率)、特效性、精度、F和$F_p$。当感兴趣的主类占少数时,过分依赖准确率度量可能受骗。 分类器的构造与评估需要把标记的数据集划分成训练集和检验集。保持、随机抽样、交叉验证和自助法都是用于这种划分的典型方法。 显著性检验和ROC曲线对于模型选择是有用的。显著性检验可以用来评估两个分类器准确率的差别是否出于偶然。ROC曲线绘制一个或多个分类器的真正例率(或灵敏性)与假正例率(或1-specificity)。 组合方法可以通过学习和组合一系列个体(基)分类器模型提高总体准确率。装袋、提升和随机森林都是流行的组合方法。 当感兴趣的主类只有少量元组代表时就会出现类不平衡问题。处理这一问题的策略包括过抽样、欠抽样、阈值移动和组合技术。  Chapter 9  不像朴素贝叶斯分类(它假定类条件独立性),贝叶斯信念网络允许在变量子集之间定义类条件独立性。它提供了一种因果关系的图形模型,在其上进行学习。训练后的贝叶斯信念网络可以用来分类。 后向传播是一种用于分类的使用梯度下降法的神经网络算法。它搜索一组权重,对数据建模,使得数据元组的网络类预测和实际类标号之间的平均平方距离最小。可以从训练过的神经网络提取规则,帮助改进学习网络的可解释性。 支持向量机(SVM)是一种用于线性和非线性数据的分类算法。它把源数据变换到较高维空间使用称做支持向量的基本元组,从中发现分离数据的超平面。 频繁模式反映数据中属性一值对(或项)之间的强关联,可以用于基于频繁模式的分类。方法包括关联分类和基于有区别能力的频繁模式分类。在关联分类中,使用从频繁模式产生的关联规则构建分类器。在基于有区别能力的频繁模式分类中,在建立分类模型时,除考虑单个特征之外,频繁模式充当组合特征。 决策树分类、贝叶斯分类、后向传播分类、支持向量机和基于频繁模式的分类方法都是急切学习方法的例子,因为它们都使用训练元组构造一个泛化模型,从而为新元组的分类做好准备。这与诸如最近邻分类和基于案例的推理分类等惰性学习方法或基于实例的方法相反。后者将所有训练元组存储在模式空间中,一直等到检验元组出现才进行泛化。因此,惰性学习方法需要有效的索引技术。 在遗传算法中,规则总体通过交叉和变异操作“进化”,直到总体中所有的规则都满足指定的阈值。粗糙集理论可以用来近似地定义类,这些类基于可用的属性是不可区分的。模糊集方法用隶属度函数替换连续值属性的“脆弱的”阈值。 可以调整二元分类方法(如支持向量机),处理多类分类。这涉及构造二元分类器的组合分类器。可以使用纠错码提高组合分类器的准确率。 当存在大量无标号的数据时,半监督学习是有用的。半监督学习使用有标号和无标号数据建立分类器。半监督分类的例子包括自我训练和协同训练。 主动学习是一种监督学习,它适合数据丰富、但类标号稀缺或难以获得的情况。学习算法可以主动地向用户(例如,智者)询问类标号。为了保持低代价,主动学习的目标是使用尽可能少的有标号的实例来获得高准确率。 迁移学习旨在从一个或多个源任务提取知识,并把这些知识运用于目标任务。TrAdaBoost是进行迁移学习的基于实例方法的一个例子,它对来自源任务的某些元组重新加权，并使用它们学习目标任务,因此只需要很少有标号的目标任务元组。  </description>
    </item>
    
    <item>
      <title>Decision Trees：Practise</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_5_practise/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_5_practise/</guid>
      <description>决策树与随机森林 决策树节点字段的选择和阈值的选择 决策树模型介绍  图中的决策树呈现自顶向下的生长过程，深色的椭圆表示树的根节点;浅色的椭圆表示树的中间节点;方框则表示树的叶节点。 对于所有的非叶节点来说，都是用来表示条件判断，而叶节点则存储最终的分类结果，例如中年分支下的叶节点(4,0)表示4位客户购买，0位客户不购买。  信息熵与条件熵  熵原本是物理学中的一个定义，后来香农将其引申到了信息论领域，用来表示信息量的大小。 信息量越大(分类越不“纯净”)，对应的熵值就越大，反之亦然。信息熵的计算公式如下:   在实际应用中，会将概率$P_k$的值用经验概率替换，所以经验信息熵可以表示为：   举例:以产品是否被购买为例，假设数据集一共包含14个样本，其中购买的用户有9个，没有购买 的用户有5个，所以对于是否购买这个事件来说，它的经验信息熵为:   条件熵  信息增益  信息增益：对于已知的事件A来说，事件D的信息增益就是D的信息熵与A事件下D的条件熵之差，事件A对于事件D的影响越大，条件熵H（D｜A）就会越小（在事件A的影响下，事件D就划分得越“纯净”），体现在信息增益熵就是差值越大，进而说明事件D的信息熵下降得越多。   所以，在根节点或中间节点的变量选择过程中，就是挑选出各自变量下因变量的信息增益最大的。  信息增益率  决策树中的ID3算法使用信息增益指标实现根节点或中间节点的字段选择，但是该指标存在一个非常明显的缺点，即信息增益会偏向于取值较多的字段。 为了克服信息增益指标的缺点，提出了信息增益率的概念，它的思想很简单，就是在信息增益的基础上进行相应的惩罚。信息增益率的公式可以表示为:   其中$H_A$为事件A的信息熵。事件A的取值越多$Gain_A$(D)可能越大，但同时$H_A$也会越大，这样以商的形式就实现了$Gain_A$(D)的惩罚。  基尼指数与条件基尼指数  决策树中的C4.5算法使用信息增益率指标实现根节点或中间节点的字段选择，但该算法与ID3算法一致，都只能针对离散型因变量进行分类，对于连续型的因变量就显得束手无策了。 为了能够让决策树预测连续型的因变量，Breiman等人在1984年提出了CART算法，该算法也称为分类回归树，它所使用的字段选择指标是基尼指数。   条件基尼指数  基尼指数增益  与信息增益类似，还需要考虑自变量对因变量的影响程度，即因变量的基尼指数下降速度的快慢，下降得越快，自变量对因变量的影响就越强。下降速度的快慢可用下方式子衡量:  生成算法与划分标准 随机森林的思想 分类树与回归树 分类树  以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&amp;lt;=阈值，和feature&amp;gt;阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 总结：分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。  回归树  回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 总结：回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值  决策树模型函数说明 DecisionTreeClassifier(criterion=&amp;#39;gini&amp;#39;, splitter=&amp;#39;best&amp;#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, class_weight=None)  criterion:用于指定选择节点字段的评价指标，对于分类决策树，默认为&amp;rsquo;gini&#39;，表示采用基尼指数选择节点的最佳分割字段;对于回归决策树，默认为&amp;rsquo;mse&#39;，表示使用均方误差选择节点的最佳分割字段 splitter:用于指定节点中的分割点选择方法，默认为&amp;rsquo;best&#39;，表示从所有的分割点中选择最佳分割点; 如果指定为&amp;rsquo;random&#39;，则表示随机选择分割点 max_depth:用于指定决策树的最大深度，默认为None，表示树的生长过程中对深度不做任何限制 min_samples_split:用于指定根节点或中间节点能够继续分割的最小样本量，默认为2 min_samples_leaf:用于指定叶节点的最小样本量，默认为1 max_leaf_nodes:用于指定最大的叶节点个数，默认为None，表示对叶节点个数不做任何限制 class_weight:用于指定因变量中类别之间的权重，默认为None，表示每个类别的权重都相等;如果为balanced，则表示类别权重与原始样本中类别的比例成反比;还可以通过字典传递类别之间的权重差异，其形式为{class_label:weight}  随机森林模型介绍  利用Bootstrp抽样法，从原始数据集合中生成k个数据集，并且每个数据集都含有N个观测和P个自变量； 针对每一个数据集，构造一棵CART决策树，在构建子树的过程中，并没有将所有自变量用作节点字段的选择，而是随机选择p个字段； 让每一棵决策树尽可能地充分生长，使得树中的每个节点尽可能“纯净”，即随机森林中的每一棵子树都不需要剪枝； 针对k棵CART树的随机森林，对分类问题利用投票法，将最高得分的类别用于最终的判断结果；对回归问题利用均值法，将其用作预测样本的最终结果。  随机森林模型函数说明 RandomForestClassifier(n_estimators=10, criterion=&amp;#39;gini&amp;#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, bootstrap=True, class_weight=None)  n_estimators:用于指定随机森林所包含的决策树个数 criterion:用于指定每棵决策树节点的分割字段所使用的度量标准，用于分类的随机森林，默认的 criterion值为&amp;rsquo;gini&#39;; 用于回归的随机森林，默认的criterion值为&amp;rsquo;mse&#39; max_depth:用于指定每棵决策树的最大深度，默认不限制树的生长深度 min_samples_split:用于指定每棵决策树根节点或中间节点能够继续分割的最小样本量，默认为2 min_samples_leaf:用于指定每棵决策树叶节点的最小样本量，默认为1 max_leaf_nodes:用于指定每棵决策树最大的叶节点个数，默认为None，表示对叶节点个数不做任 何限制 bootstrap:bool类型参数，是否对原始数据集进行bootstrap抽样，用于子树的构建，默认为True class_weight:用于指定因变量中类别之间的权重，默认为None，表示每个类别的权重都相等  代码实操 导入数据 # 导入第三方模块 import pandas as pd # 读入数据 Titanic = pd.</description>
    </item>
    
    <item>
      <title>Decision Trees：Project</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_6_project/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_6_project/</guid>
      <description>随机森林模型项目代码 建模版 导入数据 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import numpy as nps from scipy import stats from sklearn import metrics # 导入Excel文档 data=pd.read_excel(&amp;#39;D:\\Britney\\项目1\\K数据.xlsx&amp;#39;) 数据预处理 # 去除为0的行 a=data[data[&amp;#39;大货唛架 YY&amp;#39;]==0].index data1=data.drop(a,axis=0) data1[data1[&amp;#39;大货唛架 YY&amp;#39;].isin([0])] y1=data1[&amp;#39;大货唛架利用率%&amp;#39;] y2=data1[&amp;#39;大货唛架 YY&amp;#39;] #如果缺失率达到40%就可以去除该因子 d1=data1.isnull().sum()/data1.shape[0] #a=d1&amp;gt;0.5 a1=pd.DataFrame(d1) print(a1[a1[0]&amp;gt;0.4]) #去掉缺失率过大的因子 data2=data1.drop([&amp;#39;后整方式&amp;#39;,&amp;#39;循环尺寸标准_经向(Inch)&amp;#39;,&amp;#39;循环尺寸标准_纬向(Inch)&amp;#39;],axis=1) # 类型变量集合 quality=[attr for attr in data2.columns if data2.dtypes[attr] == &amp;#39;object&amp;#39;] # 类型变量缺失值补全 for c in quality: data2[c] = data2[c].</description>
    </item>
    
  </channel>
</rss>
