<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Practise on M1sty</title>
    <link>https://www.m1sty.com/tags/practise/</link>
    <description>Recent content in Practise on M1sty</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.m1sty.com/tags/practise/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Decision Trees：Practise</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_5_practise/</link>
      <pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_5_practise/</guid>
      <description>决策树与随机森林 决策树节点字段的选择和阈值的选择 决策树模型介绍  图中的决策树呈现自顶向下的生长过程，深色的椭圆表示树的根节点;浅色的椭圆表示树的中间节点;方框则表示树的叶节点。 对于所有的非叶节点来说，都是用来表示条件判断，而叶节点则存储最终的分类结果，例如中年分支下的叶节点(4,0)表示4位客户购买，0位客户不购买。  信息熵与条件熵  熵原本是物理学中的一个定义，后来香农将其引申到了信息论领域，用来表示信息量的大小。 信息量越大(分类越不“纯净”)，对应的熵值就越大，反之亦然。信息熵的计算公式如下:   在实际应用中，会将概率$P_k$的值用经验概率替换，所以经验信息熵可以表示为：   举例:以产品是否被购买为例，假设数据集一共包含14个样本，其中购买的用户有9个，没有购买 的用户有5个，所以对于是否购买这个事件来说，它的经验信息熵为:   条件熵  信息增益  信息增益：对于已知的事件A来说，事件D的信息增益就是D的信息熵与A事件下D的条件熵之差，事件A对于事件D的影响越大，条件熵H（D｜A）就会越小（在事件A的影响下，事件D就划分得越“纯净”），体现在信息增益熵就是差值越大，进而说明事件D的信息熵下降得越多。   所以，在根节点或中间节点的变量选择过程中，就是挑选出各自变量下因变量的信息增益最大的。  信息增益率  决策树中的ID3算法使用信息增益指标实现根节点或中间节点的字段选择，但是该指标存在一个非常明显的缺点，即信息增益会偏向于取值较多的字段。 为了克服信息增益指标的缺点，提出了信息增益率的概念，它的思想很简单，就是在信息增益的基础上进行相应的惩罚。信息增益率的公式可以表示为:   其中$H_A$为事件A的信息熵。事件A的取值越多$Gain_A$(D)可能越大，但同时$H_A$也会越大，这样以商的形式就实现了$Gain_A$(D)的惩罚。  基尼指数与条件基尼指数  决策树中的C4.5算法使用信息增益率指标实现根节点或中间节点的字段选择，但该算法与ID3算法一致，都只能针对离散型因变量进行分类，对于连续型的因变量就显得束手无策了。 为了能够让决策树预测连续型的因变量，Breiman等人在1984年提出了CART算法，该算法也称为分类回归树，它所使用的字段选择指标是基尼指数。   条件基尼指数  基尼指数增益  与信息增益类似，还需要考虑自变量对因变量的影响程度，即因变量的基尼指数下降速度的快慢，下降得越快，自变量对因变量的影响就越强。下降速度的快慢可用下方式子衡量:  生成算法与划分标准 随机森林的思想 分类树与回归树 分类树  以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&amp;lt;=阈值，和feature&amp;gt;阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。 总结：分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。  回归树  回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 总结：回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值  决策树模型函数说明 DecisionTreeClassifier(criterion=&amp;#39;gini&amp;#39;, splitter=&amp;#39;best&amp;#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, class_weight=None)  criterion:用于指定选择节点字段的评价指标，对于分类决策树，默认为&amp;rsquo;gini&#39;，表示采用基尼指数选择节点的最佳分割字段;对于回归决策树，默认为&amp;rsquo;mse&#39;，表示使用均方误差选择节点的最佳分割字段 splitter:用于指定节点中的分割点选择方法，默认为&amp;rsquo;best&#39;，表示从所有的分割点中选择最佳分割点; 如果指定为&amp;rsquo;random&#39;，则表示随机选择分割点 max_depth:用于指定决策树的最大深度，默认为None，表示树的生长过程中对深度不做任何限制 min_samples_split:用于指定根节点或中间节点能够继续分割的最小样本量，默认为2 min_samples_leaf:用于指定叶节点的最小样本量，默认为1 max_leaf_nodes:用于指定最大的叶节点个数，默认为None，表示对叶节点个数不做任何限制 class_weight:用于指定因变量中类别之间的权重，默认为None，表示每个类别的权重都相等;如果为balanced，则表示类别权重与原始样本中类别的比例成反比;还可以通过字典传递类别之间的权重差异，其形式为{class_label:weight}  随机森林模型介绍  利用Bootstrp抽样法，从原始数据集合中生成k个数据集，并且每个数据集都含有N个观测和P个自变量； 针对每一个数据集，构造一棵CART决策树，在构建子树的过程中，并没有将所有自变量用作节点字段的选择，而是随机选择p个字段； 让每一棵决策树尽可能地充分生长，使得树中的每个节点尽可能“纯净”，即随机森林中的每一棵子树都不需要剪枝； 针对k棵CART树的随机森林，对分类问题利用投票法，将最高得分的类别用于最终的判断结果；对回归问题利用均值法，将其用作预测样本的最终结果。  随机森林模型函数说明 RandomForestClassifier(n_estimators=10, criterion=&amp;#39;gini&amp;#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_leaf_nodes=None, bootstrap=True, class_weight=None)  n_estimators:用于指定随机森林所包含的决策树个数 criterion:用于指定每棵决策树节点的分割字段所使用的度量标准，用于分类的随机森林，默认的 criterion值为&amp;rsquo;gini&#39;; 用于回归的随机森林，默认的criterion值为&amp;rsquo;mse&#39; max_depth:用于指定每棵决策树的最大深度，默认不限制树的生长深度 min_samples_split:用于指定每棵决策树根节点或中间节点能够继续分割的最小样本量，默认为2 min_samples_leaf:用于指定每棵决策树叶节点的最小样本量，默认为1 max_leaf_nodes:用于指定每棵决策树最大的叶节点个数，默认为None，表示对叶节点个数不做任 何限制 bootstrap:bool类型参数，是否对原始数据集进行bootstrap抽样，用于子树的构建，默认为True class_weight:用于指定因变量中类别之间的权重，默认为None，表示每个类别的权重都相等  代码实操 导入数据 # 导入第三方模块 import pandas as pd # 读入数据 Titanic = pd.</description>
    </item>
    
    <item>
      <title>Decision Trees：Project</title>
      <link>https://www.m1sty.com/2021/dm_decision-trees_6_project/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_decision-trees_6_project/</guid>
      <description>随机森林模型项目代码 建模版 导入数据 import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import numpy as nps from scipy import stats from sklearn import metrics # 导入Excel文档 data=pd.read_excel(&amp;#39;D:\\Britney\\项目1\\K数据.xlsx&amp;#39;) 数据预处理 # 去除为0的行 a=data[data[&amp;#39;大货唛架 YY&amp;#39;]==0].index data1=data.drop(a,axis=0) data1[data1[&amp;#39;大货唛架 YY&amp;#39;].isin([0])] y1=data1[&amp;#39;大货唛架利用率%&amp;#39;] y2=data1[&amp;#39;大货唛架 YY&amp;#39;] #如果缺失率达到40%就可以去除该因子 d1=data1.isnull().sum()/data1.shape[0] #a=d1&amp;gt;0.5 a1=pd.DataFrame(d1) print(a1[a1[0]&amp;gt;0.4]) #去掉缺失率过大的因子 data2=data1.drop([&amp;#39;后整方式&amp;#39;,&amp;#39;循环尺寸标准_经向(Inch)&amp;#39;,&amp;#39;循环尺寸标准_纬向(Inch)&amp;#39;],axis=1) # 类型变量集合 quality=[attr for attr in data2.columns if data2.dtypes[attr] == &amp;#39;object&amp;#39;] # 类型变量缺失值补全 for c in quality: data2[c] = data2[c].</description>
    </item>
    
    <item>
      <title>Clusterings：Practise(DBSCAN)</title>
      <link>https://www.m1sty.com/2021/dm_clusterings_5_practise_dbscan/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_clusterings_5_practise_dbscan/</guid>
      <description>Clusterings 熟悉密度聚类中的几个概念 模型介绍   Kmeans聚类存在两个致命缺点，一是聚类效果容易受到异常样本点的影响;二是该算法无法准确地将非球形样本进行合理的聚类。
  基于密度的聚类则可以解决非球形簇的问题，“密度”可以理解为样本点的紧密程度，如果在指定的半径领域内，实际样本量超过给定的最小样本量阈值，则认为是密度高的对象，就可以聚成一个簇。
  概念讲解  点的领域：在某点p处，给定其半径e后，所得到的覆盖区域 核心对象：对于给定的最少样本量MinPts而言，如果某点p的e领域内至少包含MinPts个样本点，则点p就为核心对象。 直接密度可达：假设点p为核心对象，且在点p的e领域内存在点q，则从点p出发到点q是直接密度可达的。 密度可达：假设存在一系列的对象链$P_1$,$P_2$,&amp;hellip;,$P_n$，如果$p_i$是关于半径e和最少样本点MinPts的直接密度可达$P_(i+1)$，则p1密度可达$P_n$。(i = 1,2,&amp;hellip;n) 密度相连：假设点o为核心对象，从点o出发得到两个密度可达点p和点q，则称点p和点q是密度相连的。 聚类的簇：簇包含了最大的密度相连所构成的样本点。 边界点：假设点p为核心对象，在其领域内包含了点b，如果点b为非核心对象，则称其为点p的边界点。 异常点：不属于任何簇的样本点。  理解密度聚类的过程 步骤讲解  为密度聚类算法设置一个合理的半径以及半径领域内所包含的最少样本量MinPts。 从数据集中随机挑选一个样本点p，检验其在半径领域内是否包含制定的最少样本量，如果包含就将其定性为核心对象，并构成一个簇C；否则，重新挑选一个样本点。 对于核心对象p所覆盖的其他样本点q，如果点q对应的半径领域内仍然包含最少样本量MinPts，就将其覆盖的样本点统统归于簇C。 重复步骤（3），将最大的密度相连所包含的样本点聚为一类，形成一个大簇。 完成步骤（4）后，重新回到步骤（2），并重复步骤（3）和（4），直到没有新的样本点可以生成新簇时算法结束。  函数介绍 cluster.DBSCAN(eps=0.5, min_samples=5, metric=‘euclidean’, p=None)   eps:用于设置密度聚类中的e领域，即半径，默认为0.5。
  min_samples:用于设置e领域内最少的样本量，默认为5。
  metric:用于指定计算点之间距离的方法，默认为欧氏距离 。
  p:当参数metric为闵可夫斯基(&amp;lsquo;minkowski&amp;rsquo;)距离时，p=1，表示计算点之间的曼哈顿距离;p=2，表示计算点之间的欧式距离；该参数的默认值为2。
  密度聚类相比Kmeans聚类的优势 球形簇的情况 K-means DBSCAN 非球形簇的情况 K-means DBSCAN DBSCAN难确定半径和MinPts
密度聚类的应用实战 利用自定义球形簇数据对比DBSCAN和K-means # 导入第三方模块 import pandas as pd import numpy as np from sklearn.</description>
    </item>
    
    <item>
      <title>Clusterings：Practise(Hierarchical Clusterings)</title>
      <link>https://www.m1sty.com/2021/dm_clusterings_5_practise_hierarchical/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_clusterings_5_practise_hierarchical/</guid>
      <description>层次聚类 理论说明 https://blog.csdn.net/shulianghan/article/details/105960850
https://blog.csdn.net/liujh845633242/article/details/103679724
层次聚类更适合小样本；K-Means更适合大样本。
代码实现 # 导入第三方模块 import pandas as pd import numpy as np from sklearn.datasets import make_blobs import matplotlib.pyplot as plt import seaborn as sns from sklearn import cluster # 构造两个球形簇的数据样本点 X,y = make_blobs(n_samples = 2000, centers = [[-1,0],[1,0.5]], cluster_std = [0.2,0.45], random_state = 1234) # 将模拟得到的数组转换为数据框，用于绘图 plot_data = pd.DataFrame(np.column_stack((X,y)), columns = [&amp;#39;x1&amp;#39;,&amp;#39;x2&amp;#39;,&amp;#39;y&amp;#39;]) # 绘制散点图（用不同的形状代表不同的簇） sns.lmplot(&amp;#39;x1&amp;#39;, &amp;#39;x2&amp;#39;, data = plot_data, hue = &amp;#39;y&amp;#39;,markers = [&amp;#39;^&amp;#39;,&amp;#39;o&amp;#39;], fit_reg = False, legend = False) # 显示图形 plt.</description>
    </item>
    
    <item>
      <title>Clusterings：Practise(K-means)</title>
      <link>https://www.m1sty.com/2021/dm_clusterings_5_practise_k-means/</link>
      <pubDate>Sat, 06 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_clusterings_5_practise_k-means/</guid>
      <description>Clusterings K-means聚类的思想和原理 模型介绍 对于监督的数据挖掘算法而言，数据集中需要包含标签变量（即因变量y的值）。但在有些场景下，并没有给定的y值，对于这类数据的建模，一般称为无监督的数据挖掘算法，最典型的当属聚类算法。
K-means聚类算法利用距离远近的思想将目标数据为制定的k个簇，进而使样本呈现簇内差异小，簇间差异大的特征。
聚类步骤  从数据中随机挑选k个样本点作为原始的簇中心 计算剩余样本与簇中心的距离，并把各样本标记为离k个簇中心最近的类别 重新计算各簇中样本点的均值，并以均值作为新的k个簇中心 不断重复第二步和第三步，直到簇中心的变化趋于稳定，形成最终的k个簇  原理介绍 最佳K值的选择 拐点法 簇内离差平方和拐点法的思想很简单，就是在不同的k值下计算簇内利差平方和，然后通过可视化的方法找到“拐点”所对应的k值。当折线图中的斜率由大突然变小时，并且之后的斜率变化缓慢，则认为突然变化的点就是寻找的目标点，因为继续随着簇数k的增加，聚类效果不再有大的变化。
def k_SSE(X,cluster): # 选择连续的K种不同的值 K = range(1,clusters+1) # 构建空列表用于存储总的簇内离差平方和 TSSE = [] for k in K: # 用于存储各个簇内离差平方和 SSE = [] kmeans = KMeans(n_clusters = k) Kmeans.fit(X) # 返回簇标签 labels = Kmeans.labels_ # 返回簇中心 centers = Kmeans.cluster_centers_ # 计算各簇样本的离差平方和，并保存到列表中 for label in set(labels): SEE.append(np.sum((X.loc[labels == label,]-centers[label,:])**2)) # 计算总的簇内离差平方和 TSSE.append(np.sum(SSE)) 轮廓系数法 该方法综合考虑了簇的密集性和分散性两个信息，如果数据集被分割为理想的k各簇，那么对应的簇内样本会很密集，而簇间样本会很分散，轮廓系数的计算公式可以表示为：
其中，a(i)体现了簇内的密集性，代表样本i与同簇内其他样本点距离的平均值；b(i)反映了簇间的分散性，它的计算过程是，样本i与其他非同簇样本点距离的平均值，然后从平均值中挑选出最小值。
当S(i)接近于-1时，说明样本i分配的不合理，需要将分配到其他簇中；当S(i)近似为0时，说明样本i落在了模糊地带，即簇的边界处；当S(i)近似为1时，说明样本i的分配是合理的。
# 构造自定义函数 def k_silhouette(X,clusters): K = range(2,clusters+1) # 构建空列表，用于存储不同簇数下的轮廓系数 S = [] for k in K: kmeans = KMeans(n_clusters = k) Kmenas.</description>
    </item>
    
    <item>
      <title>Association：Practise</title>
      <link>https://www.m1sty.com/2021/dm_association_5_practise/</link>
      <pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_association_5_practise/</guid>
      <description>#we run apriori on the order_data.csv file import math import pandas as pd from mlxtend.preprocessing import TransactionEncoder from mlxtend.frequent_patterns import apriori from mlxtend.frequent_patterns import association_rules data = pd.read_csv(r&amp;#34;order_data.csv&amp;#34;,delimiter=&amp;#34; &amp;#34;,header=None) #preprocessing: change to one hot encoding so as to be able to use apriori from mlxtend d=data.values.tolist() #removing nan values for i in range(len(d)): j=0 while(True): if (type(d[i][j])==float and math.isnan(d[i][j])) : del d[i][j] j-=1 j+=1 if (j&amp;gt;len(d[i])-1): break te = TransactionEncoder() te_ary = te.</description>
    </item>
    
    <item>
      <title>Regression：Ridge Regression and Lasson Regression</title>
      <link>https://www.m1sty.com/2021/dm_regression_5_pracitise_ridge-regression-and-lasso-regression/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_regression_5_pracitise_ridge-regression-and-lasso-regression/</guid>
      <description>线性回归模型的短板 当列数比行数多 当列之间存在多重共线性 岭回归模型 为解决多元线性回归模型中可能存在的不可逆问题，统计学家提出了岭回归模型。该模型解决问题的思路就是在线性回归模型的目标函数之上增加l2正则项（也称为惩罚项）。
表达式 系数求解 凸优化的等价命题 系数求解的几何意义 交叉验证法 Lasso模型 岭回归模型解决线性回归模型中矩阵X&amp;rsquo;X不可逆的办法是添加l2正则的惩罚项,但缺陷在于始终保留建模时的所有变量,无法降低模型的复杂度。对于 此, Lasso回归采用了l1正则的惩罚项。
表达式 凸优化的等价命题 系数求解的几何意义 交叉验证法 实操 # 导入第三方模块 import pandas as pd import numpy as np from sklearn import model_selection from sklearn.linear_model import Ridge,RidgeCV import matplotlib.pyplot as plt # 读取糖尿病数据集 diabetes = pd.read_excel(r&amp;#39;diabetes.xlsx&amp;#39;) # 构造自变量（剔除患者性别、年龄和因变量） predictors = diabetes.columns[2:-1] # 将数据集拆分为训练集和测试集 X_train, X_test, y_train, y_test = model_selection.train_test_split(diabetes[predictors], diabetes[&amp;#39;Y&amp;#39;], test_size = 0.2, random_state = 1234 ) 多元线性回归 # 导入第三方模块 from statsmodels import api as sms # 为自变量X添加常数列1，用于拟合截距项 X_train2 = sms.</description>
    </item>
    
    <item>
      <title>Regression：Linear Regression</title>
      <link>https://www.m1sty.com/2021/dm_regression_5_pracitise_linear-regression/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_regression_5_pracitise_linear-regression/</guid>
      <description>线性回归 一元线性回归 相关分析 一元线性回归模型  模型中的x称为自变量，y称为因变量; a为模型的截距项，b为模型的斜率项，ε为模型的误差项; 误差项ε的存在主要是为了平衡等号两边的值，通常被称为模型 无法解释的部分; 如果拟合线能够精确地捕捉到每一个点(即所有散点全部落在拟合线上)，那么对应的误差项ε应该为0; 所以，模型拟合的越好，则误差项ε应该越小。进而可以理解为: 求解参数的问题便是求解误差平方和最小的问题。  参数a和b求解  J(a,b)为目标函数，需求解该函数的最小值； 求解方法便是计算目标函数关于参数a和b的两个偏导数，最终令偏倒数为0即可。  模型的应用 # 导入第三方模块 import statsmodels.api as sm sm.ols(formula, data, subset=None, drop_cols=None)  formula:以字符串的形式指定线性回归模型的公式，如&amp;rsquo;y~x&amp;rsquo;就表示简单线性回归模型 data:指定建模的数据集 subset:通过bool类型的数组对象，获取data的子集用于建模 drop_cols:指定需要从data中删除的变量  # 导入第三方模块 import pandas as pd import statsmodels.api as sm # 导入数据 income = pd.read_csv(&amp;#39;Salary_Data.csv&amp;#39;) # 利用收入数据集，构建回归模型 fit = sm.formula.ols(&amp;#39;Salary ~ YearsExperience&amp;#39;, data = income).fit() # 返回模型的参数值 fit.params # 相关系数 income.columns income.Salary.corr(income.YearsExperience) 多元线性回归 定义  对于一元线性回归模型来说，其反映的是单个自变量对因变量的影响，然而实际情况中，影 响因变量的自变量往往不止一个，从而需要将一元线性回归模型扩展到多元线性回归模型。  参数求解 模型的应用  数据集包含5个变量，分别是产品的研发成本、管理成本、市场营销成本、销售市场和销 售利润。  # 导入模块 from sklearn import model_selection # 导入数据 Profit = pd.</description>
    </item>
    
    <item>
      <title>Pre-processing</title>
      <link>https://www.m1sty.com/2021/dm_0_clear/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.m1sty.com/2021/dm_0_clear/</guid>
      <description>常用工具 numpy 构建数组   Numpy中常用的数据结构是ndarray格式
  使用array函数创建，语法格式为array(列表或元组)
  可以使用其他函数例如arange、linspace、zeros等创建
  import numpy as np arr1 = np.array([-9,7,4,3]) np.arange(0,10,1) np.linspace(1,10,10) np.zeros (1,5) 常用方法   常用方法名称：ndim、shape、size、dtype、运算
  数组访问方法：array$[行，列]$
  排序  sort函数：从小到大进行排序 argsort函数：返回的是数据中, 从小到大的索引值  s = np.array([1,2,3,4,3,1,2,2,4,6,7,2,4,8,4,5]) np.sort(s) np.argsort(s) 搜索 np.where np.extract np.where(s&amp;gt;3,1,-1) np.extract(s&amp;gt;3,s) pandas 构建数组（series） series1 = pd.Series([2.8,3.01,8.99,8.59,5.18]) series2 = pd.Series([2.8,3.01,8.99,8.59,5.18],index = [&amp;#39;a&amp;#39;,&amp;#39;b&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;d&amp;#39;,&amp;#39;e&amp;#39;],name =&amp;#39;这是一个series’)  series3 = pd.Series(np.array((2.8,3.10,8.99,8.59,5.18)),index = [&amp;#39;a&amp;#39;,&amp;#39;b&amp;#39;,&amp;#39;c&amp;#39;,&amp;#39;d&amp;#39;,&amp;#39;e’]) series4 = pd.Series({&amp;#39;北京&amp;#39;:2.8,&amp;#39;上海&amp;#39;:3.01,&amp;#39;广东&amp;#39;:8.99,&amp;#39;江苏&amp;#39;:8.59,&amp;#39;浙江&amp;#39;:5.18}) 构建数组（dataframe） list1 = [[&amp;#39;张三&amp;#39;,23,&amp;#39;男&amp;#39;],[&amp;#39;李四&amp;#39;,27,&amp;#39;女&amp;#39;],[&amp;#39;王二&amp;#39;,26,&amp;#39;女&amp;#39;]]#使用嵌套列表 df1 = pd.</description>
    </item>
    
  </channel>
</rss>
